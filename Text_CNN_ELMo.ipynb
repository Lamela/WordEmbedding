{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un CNN classifieur pour utiliser ELMo. Implémentation de version Tensorflow. Les codes de CNN sont basé sur https://github.com/gaussic/text-classification-cnn-rnn et la partie d'intégration de ELMo bilm est modifié depuis les scripts sur [bilm-tf](https://github.com/allenai/bilm-tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import re\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, LSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils, Sequence\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_util import plot_confusion_matrix\n",
    "import time\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "sys.path.append(\"..\")\n",
    "    \n",
    "from bilm_tf.bilm import TokenBatcher, BidirectionalLanguageModel, \\\n",
    "weight_layers, dump_token_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(s):\n",
    "    \"\"\"Output only Chinese, English characters and Arabic numerals\"\"\"\n",
    "    filtrate = re.compile(u'[^\\u0030-\\u0039\\u0041-\\u005A\\u0061-\\u007A\\u4E00-\\u9FA5]')\n",
    "    res = filtrate.sub(r' ', s)\n",
    "    return res\n",
    "\n",
    "def read_datasets(data_path):\n",
    "    \"\"\"read datasets form files\"\"\"\n",
    "    x, y = [], []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                content, label = line.strip().split('__label__')\n",
    "                if content:\n",
    "                    x.append(filter(content).replace('  ', ''))\n",
    "                    y.append(label.replace('\\n', ''))\n",
    "            except:\n",
    "                pass\n",
    "    print(len(x))\n",
    "    return x, y\n",
    "\n",
    "def read_vocab(vocab_dir):\n",
    "    with open(vocab_dir, 'r') as f:\n",
    "        words = [_.strip() for _ in f.readlines()]\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "    return words, word_to_id\n",
    "\n",
    "def read_category(data_path):\n",
    "    \"\"\"get classes list and corresponding id list\"\"\"\n",
    "    x, y = read_datasets(data_path)\n",
    "    categories = np.unique(y)\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    return categories, cat_to_id\n",
    "\n",
    "def process_datasets(data_path, word_to_id, cat_to_id, max_length, vocab_size):\n",
    "    \"\"\"transform words and labels to id, pad the examples\"\"\"\n",
    "    print('Converting data to trainable form...')\n",
    "    contents, labels = read_datasets(data_path)\n",
    "    \n",
    "    data_id, label_id = [], []\n",
    "    for i in range(len(contents)):\n",
    "        data_id.append([word_to_id[x] for x in contents[i] if x in word_to_id])\n",
    "        label_id.append(cat_to_id[labels[i]])\n",
    "        \n",
    "    x_pad = sequence.pad_sequences(data_id, maxlen=max_length)\n",
    "    y_pad = keras.utils.to_categorical(label_id, num_classes=len(cat_to_id))\n",
    "    print('Number of examples counted by x_pad: ' + str(len(x_pad)))\n",
    "    print('Number of examples counted by y_pad: ' + str(len(y_pad)))\n",
    "    \n",
    "    return x_pad, y_pad\n",
    "\n",
    "def batch_iter(x, y, batch_size=128):\n",
    "    \"\"\"generate batch data\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = (data_len - 1) // batch_size\n",
    "#     if (data_len - 1) % batch_size != 0:\n",
    "#         num_batch = num_batch + 1\n",
    "    \n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indices]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN parrameters config\"\"\"\n",
    "    embedding_dim = 600  # embedding_size\n",
    "    seq_length = 1002 # max length for each news\n",
    "    num_classes = 13  \n",
    "    num_filters = 32  # number of filters\n",
    "    kernel_size = 5  # conv kernel size\n",
    "    vocab_size = 5000\n",
    "\n",
    "    hidden_dim = 512  # number of units in fc1\n",
    "    \n",
    "    dropout_keep_prob = 0.5 \n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    batch_size = 64  \n",
    "    num_epochs = 2  \n",
    "\n",
    "    print_per_batch = 100  \n",
    "    save_per_batch = 10  # save into tensorboard\n",
    "    \n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"Text Classification, CNN Model\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # inputs\n",
    "        self.input_x = tf.placeholder(tf.float32,\n",
    "                                      [None, self.config.seq_length - 2, self.config.embedding_dim], \n",
    "                                      name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32,\n",
    "                                      [None, self.config.num_classes],\n",
    "                                      name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.cnn()\n",
    "    \n",
    "    def cnn(self):\n",
    "        \"\"\"CNN Model\"\"\"\n",
    "        \n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # inputs shape : [batch, filter_width, in_channels]\n",
    "            # filters shape : [filter_width, in_channels, out_channels]\n",
    "            \n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(self.input_x, \n",
    "                                    filters=32,\n",
    "                                    kernel_size=5,\n",
    "                                    strides=1, \n",
    "                                    padding='same',\n",
    "                                    name='conv')\n",
    "            conv_relu = tf.nn.relu(conv)\n",
    "            conv_dropout = tf.contrib.layers.dropout(conv_relu, keep_prob=self.keep_prob)\n",
    "            \n",
    "            # max pooling layer\n",
    "            mpl = tf.layers.max_pooling1d(conv_dropout,\n",
    "                                          pool_size=2,\n",
    "                                          strides=1,\n",
    "                                          padding='valid',\n",
    "                                          name='mpl')\n",
    "        \n",
    "        with tf.name_scope(\"score\"):\n",
    "            fc = tf.layers.flatten(mpl)\n",
    "            fc = tf.layers.dense(fc, units=512, activation=tf.nn.relu, name='fc1')\n",
    "            fc_dropout = tf.contrib.layers.dropout(fc, keep_prob=self.keep_prob)\n",
    "            self.logits = tf.layers.dense(fc_dropout, units=13, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(self.logits), 1)\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # loss function, cross entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                    labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # opptimizer\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "121155\n",
      "WARNING:tensorflow:From /home/chenxin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "WARNING:tensorflow:From /home/chenxin/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:198: retry (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use the retry module or similar alternatives.\n",
      "WARNING:tensorflow:From <ipython-input-3-37c25f491925>:71: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See tf.nn.softmax_cross_entropy_with_logits_v2.\n",
      "\n",
      "USING SKIP CONNECTIONS\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "\n",
    "base_dir = 'datasets/'\n",
    "train_dir = os.path.join(base_dir, 'news_fasttext_train.txt')\n",
    "test_dir = os.path.join(base_dir, 'news_fasttext_test.txt')\n",
    "vocab_dir = 'pre_trained_vectors/raw_300/ft_5_vocab.txt'\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "vocab_file = '/home/chenxin/WordEmbedding/pre_trained_vectors/raw_300/ft_5_vocab.txt'\n",
    "options_file = '/home/chenxin/ELMo/pure_checkpoint/options.json'\n",
    "weight_file = '/home/chenxin/ELMo/pure_checkpoint/weights.hdf5'\n",
    "token_embedding_file = '/home/chenxin/WordEmbedding/pre_trained_vectors/raw_300/vocab_embedding.hdf5'\n",
    "prefix = 'pre_trained_vectors/elmo_600/elmo_vector_'\n",
    "\n",
    "with open(options_file, 'r') as fin:\n",
    "    options = json.load(fin)\n",
    "\n",
    "tf.reset_default_graph()\n",
    "categories, cat_to_id = read_category(train_dir)\n",
    "config = TCNNConfig()\n",
    "words, word_to_id = read_vocab(vocab_dir)\n",
    "config.vocab_size = len(words)\n",
    "model = TextCNN(config)\n",
    "## Now we can do inference.\n",
    "# Create a TokenBatcher to map text to token ids.\n",
    "batcher = TokenBatcher(vocab_file)\n",
    "\n",
    "# Input placeholders to the biLM.\n",
    "context_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "\n",
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(\n",
    "    options_file,\n",
    "    weight_file,\n",
    "    use_character_inputs=False,\n",
    "    embedding_weight_file=token_embedding_file\n",
    ")\n",
    "\n",
    "# Get ops to compute the LM embeddings.\n",
    "context_embeddings_op = bilm(context_token_ids)\n",
    "\n",
    "#with tf.variable_scope('', reuse=True):\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    "\n",
    "#with tf.variable_scope('', reuse=True):\n",
    "elmo_context_output = weight_layers('output', context_embeddings_op, l2_coef=0.0)\n",
    "\n",
    "def get_time_diff(start_time):\n",
    "    \"\"\"get time passed\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_diff)))\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"evaluate the accuracy and loss on an example\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # config Tensorboard，delete tensorboard folder for re-training\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    # config saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    print(\"Loading training and validation data...\")\n",
    "    start_time = time.time()\n",
    "    x, y = process_datasets(train_dir, word_to_id, cat_to_id,\n",
    "                            max_length=config.seq_length,\n",
    "                            vocab_size=config.vocab_size)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n",
    "    time_diff = get_time_diff(start_time)\n",
    "    print(\"Time usage:\", time_diff)\n",
    "    \n",
    "    # Create session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(sess.graph)\n",
    "\n",
    "        print(\"Training and evaluating...\")\n",
    "        start_time = time.time()\n",
    "        total_batch = 0 # total batch\n",
    "        best_acc_val = 0.0 # best validation set accurary\n",
    "        last_improved = 0 # record last improved batch\n",
    "        # if more than 1000 batches, we don't get improvement, we terminate training phase\n",
    "        require_improvement = 1000\n",
    "        \n",
    "        flag = False\n",
    "        for epoch in range(config.num_epochs):\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "            for x_batch, y_batch in batch_train:\n",
    "                # TODO: add bilm context input\n",
    "                # Compute ELMo representations (here for the input only, for simplicity).\n",
    "                elmo_context_input_ = sess.run(\n",
    "                    [elmo_context_input['weighted_op']],\n",
    "                    feed_dict={context_token_ids: x_batch}\n",
    "                )\n",
    "                elmo_context_input_ = np.array(elmo_context_input_)\n",
    "                         \n",
    "                feed_dict = feed_data(elmo_context_input_[0], y_batch, config.dropout_keep_prob)\n",
    "                \n",
    "                if total_batch % config.save_per_batch == 0:\n",
    "                    s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(s, total_batch)\n",
    "                    \n",
    "                if total_batch % config.print_per_batch == 0:\n",
    "                    feed_dict[model.keep_prob] = 1.0\n",
    "                    loss_train, acc_train = sess.run([model.loss, model.acc],\n",
    "                                                     feed_dict=feed_dict)\n",
    "                    x_val_batch = x_val[total_batch // 10 * config.batch_size:(total_batch // 10 + 1) * config.batch_size]\n",
    "                    y_val_batch = y_val[total_batch // 10 * config.batch_size:(total_batch // 10 + 1) * config.batch_size]\n",
    "                    elmo_context_input_ = sess.run(\n",
    "                        [elmo_context_input['weighted_op']],\n",
    "                        feed_dict={context_token_ids: x_val_batch})\n",
    "                    loss_val, acc_val = evaluate(sess, elmo_context_input_[0], y_val_batch)\n",
    "                    if acc_val > best_acc_val:\n",
    "                        # Save the best result\n",
    "                        best_acc_val = acc_val\n",
    "                        last_improved = total_batch\n",
    "                        saver.save(sess=sess, save_path=save_path)\n",
    "                        improved_str = '*'\n",
    "                    else:\n",
    "                        improved_str = ''\n",
    "                        \n",
    "                    time_diff = get_time_diff(start_time)\n",
    "                    msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                    print(msg.format(total_batch, loss_train, acc_train, loss_val,\n",
    "                                     acc_val, time_diff, improved_str))\n",
    "                sess.run(model.optim, feed_dict=feed_dict) # run optimization\n",
    "                total_batch += 1\n",
    "                \n",
    "                if total_batch - last_improved > require_improvement:\n",
    "                    print(\"No optimization for a long time, auto-stopping...\")\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "                \n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_datasets(test_dir, cat_to_id, config.seq_length, config.vocab_size)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "        \n",
    "        print(\"Testing...\")\n",
    "        elmo_context_input_ = sess.run(\n",
    "            [elmo_context_input['weighted_op']],\n",
    "            feed_dict={context_token_ids: x_batch}\n",
    "        )\n",
    "        loss_test, acc_test = evaluate(sess, elmo_context_input_[0], y_test)\n",
    "        msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "        print(msg.format(loss_test, acc_test))\n",
    "        \n",
    "        batch_size = config.batch_size\n",
    "        data_len = len(x_test)\n",
    "        num_batch = (data_len - 1) / batch_size\n",
    "        if (data_len - 1) % batch_size != 0:\n",
    "            num_batch += 1\n",
    "        \n",
    "        y_test_cls = np.argmax(y_test, 1)\n",
    "        y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)\n",
    "        for i in range(num_batch):\n",
    "            start_id = i * batch_size\n",
    "            end_id = ((i + 1) * batch_size, data_len)\n",
    "            elmo_context_input_ = sess.run(\n",
    "                    [elmo_context_input['weighted_op']],\n",
    "                    feed_dict={context_token_ids: x_test[start_id:end_id]}\n",
    "                )\n",
    "            feed_dict = {\n",
    "                model.input_x: elmo_context_input_,\n",
    "                model.keep_prob: 1.0\n",
    "            }\n",
    "            y_pred_cls[start_id:end_id] = sess.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        \n",
    "        print(\"Precision, Recall and F1-Score...\")\n",
    "        print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "        \n",
    "        print(\"Confusion Matrix...\")\n",
    "        cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "        print(cm)\n",
    "        \n",
    "        time_diff = get_time_diff(start_time)\n",
    "        print(\"Time usage: \", time_diff)\n",
    "\n",
    "        \n",
    "def run_cnn(argv):\n",
    "    if argv not in ['train', 'test']:\n",
    "        raise ValueError(\"Wrong argument.\")\n",
    "        \n",
    "    print(\"Configuring CNN Model...\")\n",
    "    \n",
    "    if argv == 'train':\n",
    "        train()\n",
    "    else:\n",
    "        test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuring CNN Model...\n",
      "Configuring TensorBoard and Saver...\n",
      "Loading training and validation data...\n",
      "Converting data to trainable form...\n",
      "121155\n",
      "Number of examples counted by x_pad: 121155\n",
      "Number of examples counted by y_pad: 121155\n",
      "Time usage: 0:00:27\n",
      "Training and evaluating...\n",
      "Epoch: 1\n",
      "Iter:      0, Train Loss:    2.6, Train Acc:   6.25%, Val Loss:    0.0, Val Acc:   0.00%, Time: 0:00:05 \n",
      "Iter:    100, Train Loss:    2.5, Train Acc:   6.25%, Val Loss:    0.0, Val Acc:   0.00%, Time: 0:03:33 \n",
      "Iter:    200, Train Loss:    2.2, Train Acc:  20.31%, Val Loss:    0.0, Val Acc:   0.00%, Time: 0:07:01 \n",
      "Iter:    300, Train Loss:    1.8, Train Acc:  40.62%, Val Loss:    0.0, Val Acc:   0.00%, Time: 0:10:28 \n",
      "Iter:    400, Train Loss:    1.4, Train Acc:  59.38%, Val Loss:    0.0, Val Acc:   0.00%, Time: 0:13:56 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-8bc496065bf8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrun_cnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-4-5cd11d476915>\u001b[0m in \u001b[0;36mrun_cnn\u001b[0;34m(argv)\u001b[0m\n\u001b[1;32m    227\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0margv\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m         \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    231\u001b[0m         \u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-5cd11d476915>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    125\u001b[0m                 elmo_context_input_ = sess.run(\n\u001b[1;32m    126\u001b[0m                     \u001b[0;34m[\u001b[0m\u001b[0melmo_context_input\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weighted_op'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m                     \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mcontext_token_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m                 )\n\u001b[1;32m    129\u001b[0m                 \u001b[0melmo_context_input_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0melmo_context_input_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1138\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1140\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1141\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1321\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1311\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1312\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1418\u001b[0m         return tf_session.TF_Run(\n\u001b[1;32m   1419\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1420\u001b[0;31m             status, run_metadata)\n\u001b[0m\u001b[1;32m   1421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1422\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_cnn('train')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
