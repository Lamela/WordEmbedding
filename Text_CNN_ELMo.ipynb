{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un CNN classifieur pour utiliser ELMo. Implémentation de version Tensorflow. Les codes de CNN sont basé sur https://github.com/gaussic/text-classification-cnn-rnn et la partie d'intégration de ELMo bilm est modifié depuis les scripts sur [bilm-tf](https://github.com/allenai/bilm-tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast\n",
    "import re\n",
    "import os\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, LSTM\n",
    "from keras.models import Model\n",
    "from keras import backend as K\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils, Sequence\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_util import plot_confusion_matrix\n",
    "import time\n",
    "import h5py\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import gc\n",
    "sys.path.append(\"..\")\n",
    "    \n",
    "from bilm_tf.bilm import TokenBatcher, BidirectionalLanguageModel, \\\n",
    "weight_layers, dump_token_embeddings\n",
    "\n",
    "\n",
    "# Model parameters\n",
    "MAX_FEATURES = 256404\n",
    "MAX_TEXT_LENGTH = 2000\n",
    "EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 2\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(s):\n",
    "    \"\"\"Output only Chinese, English characters and Arabic numerals\"\"\"\n",
    "    filtrate = re.compile(u'[^\\u0030-\\u0039\\u0041-\\u005A\\u0061-\\u007A\\u4E00-\\u9FA5]')\n",
    "    res = filtrate.sub(r' ', s)\n",
    "    return res\n",
    "\n",
    "def read_datasets(data_path):\n",
    "    \"\"\"read datasets form files\"\"\"\n",
    "    x, y = [], []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            try:\n",
    "                content, label = line.strip().split('__label__')\n",
    "                if content:\n",
    "                    x.append(filter(content).replace('  ', ''))\n",
    "                    y.append(label.replace('\\n', ''))\n",
    "            except:\n",
    "                pass\n",
    "                \n",
    "    return x, y\n",
    "\n",
    "def read_category(data_path):\n",
    "    \"\"\"get classes list and corresponding id list\"\"\"\n",
    "    x, y = read_datasets(data_path)\n",
    "    categories = np.unique(y)\n",
    "    cat_to_id = dict(zip(categories, range(len(categories))))\n",
    "    \n",
    "    return categories, cat_to_id\n",
    "\n",
    "def process_datasets(data_path, cat_to_id, max_length=2000, vocab_size):\n",
    "    \"\"\"transform words and labels to id, pad the examples\"\"\"\n",
    "    print('Converting data to trainable form...')\n",
    "    x, y = read_datasets(data_path)\n",
    "    tokenizer = text.Tokenizer(num_words=vocab_size)\n",
    "    tokenizer.fit_on_texts(x)\n",
    "    word_index = tokenizer.word_index\n",
    "    x_tokenized = tokenizer.texts_to_sequences(x)\n",
    "    label_id = []\n",
    "    for i in range(len(x)):\n",
    "        label_id.append(cat_to_id[y[i]])\n",
    "        \n",
    "    x_pad = sequence.pad_sequences(x_tokenized, maxlen=max_length)\n",
    "    y_pad = keras.utils.to_categorical(label_id, num_classes=len(cat_to_id))\n",
    "    print('Number of examples counted by x_pad: ' + str(len(x_pad)))\n",
    "    print('Number of examples counted by y_pad: ' + str(len(y_pad)))\n",
    "    \n",
    "    return x_pad, y_pad\n",
    "\n",
    "def batch_iter(x, y, batch_size=128):\n",
    "    \"\"\"generate batch data\"\"\"\n",
    "    data_len = len(x)\n",
    "    num_batch = int((data_len - 1) / batch_size) \n",
    "    if (data_len - 1) % batch_size != 0:\n",
    "        num_batch = num_batch + 1\n",
    "    \n",
    "    indices = np.random.permutation(np.arange(data_len))\n",
    "    x_shuffle = x[indices]\n",
    "    y_shuffle = y[indicies]\n",
    "    \n",
    "    for i in range(num_batch):\n",
    "        start_id = i * batch_size\n",
    "        end_id = min((i + 1) * batch_size, data_len)\n",
    "        yield x_shuffle[start_id:end_id], y_shuffle[start_id:end_id]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNNConfig(object):\n",
    "    \"\"\"CNN parrameters config\"\"\"\n",
    "    embedding_dim = 600  # embedding_size\n",
    "    seq_length = 2000  # max length for each news\n",
    "    num_classes = 13  \n",
    "    num_filters = 32  # number of filters\n",
    "    kernel_size = 5  # conv kernel size\n",
    "    vocab_size = \n",
    "\n",
    "    hidden_dim = 512  # number of units in fc1\n",
    "    \n",
    "    dropout_keep_prob = 0.5 \n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    batch_size = 128  \n",
    "    num_epochs = 2  \n",
    "\n",
    "    print_per_batch = 100  \n",
    "    save_per_batch = 10  # save into tensorboard\n",
    "    \n",
    "\n",
    "class TextCNN(object):\n",
    "    \"\"\"Text Classification, CNN Model\"\"\"\n",
    "    \n",
    "    def __init__(self, config):\n",
    "        self.config = config\n",
    "        # inputs\n",
    "        self.input_x = tf.placeholder(tf.float32,\n",
    "                                      [None, self.config.seq_length, self.config.embedding_dim], \n",
    "                                      name='input_x')\n",
    "        self.input_y = tf.placeholder(tf.float32,\n",
    "                                      [None, self.config.num_classes],\n",
    "                                      name='input_y')\n",
    "        self.keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "        self.cnn()\n",
    "    \n",
    "    def cnn(self):\n",
    "        \"\"\"CNN Model\"\"\"\n",
    "        with tf.name_scope(\"cnn\"):\n",
    "            # inputs shape : [batch, filter_width, in_channels]\n",
    "            # filters shape : [filter_width, in_channels, out_channels]\n",
    "            \n",
    "            # CNN layer\n",
    "            conv = tf.layers.conv1d(inputs, \n",
    "                                    filters=32,\n",
    "                                    kernel_size=5,\n",
    "                                    stride=1, \n",
    "                                    padding='same',\n",
    "                                    name='conv')\n",
    "            conv_relu = tf.nn.relu(conv)\n",
    "            conv_dropout = tf.contrib.layers.dropout(conv_relu, keep_prob=self.keep_prob)\n",
    "            \n",
    "            # max pooling layer\n",
    "            mpl = tf.layers.max_pooling1d(conv_dropout,\n",
    "                                          pool_size=2,\n",
    "                                          strides=1,\n",
    "                                          padding='valid',\n",
    "                                          name='mpl')\n",
    "        \n",
    "        with tf.name_scope(\"score\"):\n",
    "            fc = tf.layers.flatten(mpl)\n",
    "            fc = tf.layers.dense(fc, units=512, activation=tf.nn.relu, name='fc1')\n",
    "            fc_dropout = tf.contrib.layers.dropout(fc, keep_prob=self.keep_prob)\n",
    "            self.logits = tf.layers.dense(fc_dropout, units=13, name='fc2')\n",
    "            self.y_pred_cls = tf.argmax(tf.nn.softmax(logits), 1)\n",
    "\n",
    "        with tf.name_scope(\"optimize\"):\n",
    "            # loss function, cross entropy\n",
    "            cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits=self.logits,\n",
    "                                                                    labels=self.input_y)\n",
    "            self.loss = tf.reduce_mean(cross_entropy)\n",
    "            # opptimizer\n",
    "            self.optim = tf.train.AdamOptimizer(learning_rate=self.config.learning_rate).minimize(self.loss)\n",
    "        \n",
    "        \n",
    "        with tf.name_scope(\"accuracy\"):\n",
    "            # accuracy\n",
    "            correct_pred = tf.equal(tf.argmax(self.input_y, 1), self.y_pred_cls)\n",
    "            self.acc = tf.reduce_mean(tf.cast(correct_pred, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run CNN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from datetime import timedelta\n",
    "from sklearn import metrics\n",
    "\n",
    "base_dir = 'datasets/'\n",
    "train_dir = os.path.join(base_dir, 'train_pure_nolabel.txt')\n",
    "test_dir = os.path.join(base_dir, 'test_pure_nolabel.txt')\n",
    "vocab_dir = 'pre_trained_vectors/raw_300/'\n",
    "save_dir = 'checkpoints/textcnn'\n",
    "\n",
    "def get_time_diff(start_time):\n",
    "    \"\"\"get time passed\"\"\"\n",
    "    end_time = time.time()\n",
    "    time_diff = end_time - start_time\n",
    "    return timedelta(seconds=int(round(time_diff)))\n",
    "\n",
    "def feed_data(x_batch, y_batch, keep_prob):\n",
    "    feed_dict = {\n",
    "        model.input_x: x_batch,\n",
    "        model.input_y: y_batch,\n",
    "        model.keep_prob: keep_prob\n",
    "    }\n",
    "    return feed_dict\n",
    "\n",
    "def evaluate(sess, x_, y_):\n",
    "    \"\"\"evaluate the accuracy and loss on an example\"\"\"\n",
    "    data_len = len(x_)\n",
    "    batch_eval = batch_iter(x_, y_, 128)\n",
    "    total_loss = 0.0\n",
    "    total_acc = 0.0\n",
    "    for x_batch, y_batch in batch_eval:\n",
    "        batch_len = len(x_batch)\n",
    "        feed_dict = feed_data(x_batch, y_batch, 1.0)\n",
    "        loss, acc = sess.run([model.loss, model.acc], feed_dict=feed_dict)\n",
    "        total_loss += loss * batch_len\n",
    "        total_acc += acc * batch_len\n",
    "\n",
    "    return total_loss / data_len, total_acc / data_len\n",
    "\n",
    "def train():\n",
    "    print(\"Configuring TensorBoard and Saver...\")\n",
    "    # config Tensorboard，delete tensorboard folder for re-training\n",
    "    tensorboard_dir = 'tensorboard/textcnn'\n",
    "    if not os.path.exists(tensorboard_dir):\n",
    "        os.makedirs(tensorboard_dir)\n",
    "    \n",
    "    tf.summary.scalar(\"loss\", model.loss)\n",
    "    tf.summary.scalar(\"accuracy\", model.acc)\n",
    "    merged_summary = tf.summary.merge_all()\n",
    "    writer = tf.summary.FileWriter(tensorboard_dir)\n",
    "    \n",
    "    # config saver\n",
    "    saver = tf.train.Saver()\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "        \n",
    "    print(\"Loading training and validation data...\")\n",
    "    start_time = time.time()\n",
    "    x, y = process_datasets(train_dir, cat_to_id,\n",
    "                            max_length=config.seq_length,\n",
    "                            vocab_size=config.vocab_size)\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x, y, test_size=0.1)\n",
    "    time_diff = get_time_diff(start_time)\n",
    "    print(\"Time usage:\", time_diff)\n",
    "    \n",
    "    # Create session\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer.add_graph(session.graph)\n",
    "        \n",
    "        print(\"Training and evaluating...\")\n",
    "        start_time = time.time()\n",
    "        total_batch = 0 # total batch\n",
    "        best_acc_val = 0.0 # best validation set accurary\n",
    "        last_improved = 0 # record last improved batch\n",
    "        # if more than 1000 batches, we don't get improvement, we terminate training phase\n",
    "        require_improvement = 1000\n",
    "        \n",
    "        flag = False\n",
    "        for epoch in range(config.num_epochs):\n",
    "            print(\"Epoch:\", epoch + 1)\n",
    "            batch_train = batch_iter(x_train, y_train, config.batch_size)\n",
    "            for x_batch, y_batch in batch_train:\n",
    "                \n",
    "                # TODO: add bilm context input\n",
    "                \n",
    "                feed_dict = feed_data(x_batch, y_batch, config.dropout_keep_prob)\n",
    "                \n",
    "                if total_batch % config.save_per_batch == 0:\n",
    "                    s = sess.run(merged_summary, feed_dict=feed_dict)\n",
    "                    writer.add_summary(s, total_batch)\n",
    "                    \n",
    "                if total_batch % config.print_per_batch == 0:\n",
    "                    feed_dict[model.keep_prob] = 1.0\n",
    "                    loss_train, acc_train = sess.run([model.loss, model.acc],\n",
    "                                                     feed_dict=feed_dict)\n",
    "                    loss_val, acc_val = evaluate(sess, x_val, y_val)\n",
    "                    \n",
    "                    if acc_val > best_acc_val:\n",
    "                        # Save the best result\n",
    "                        best_acc_val = acc_val\n",
    "                        last_improved = total_batch\n",
    "                        saver.save(sess=sess, save_path=save_path)\n",
    "                        improved_str = '*'\n",
    "                    else:\n",
    "                        improved_str = ''\n",
    "                        \n",
    "                    time_diff = get_time_diff(start_time)\n",
    "                    msg = 'Iter: {0:>6}, Train Loss: {1:>6.2}, Train Acc: {2:>7.2%},' \\\n",
    "                      + ' Val Loss: {3:>6.2}, Val Acc: {4:>7.2%}, Time: {5} {6}'\n",
    "                    print(msg.format(total_batch, loss_train, acc_train, loss_val,\n",
    "                                     acc_val, time_diff, improved_str))\n",
    "                sess.run(model.optim, feed_dict=feed_dict) # run optimization\n",
    "                total_batch += 1\n",
    "                \n",
    "                if total_batch - last_improved > require_improvement:\n",
    "                    print(\"No optimization for a long time, auto-stopping...\")\n",
    "                    flag = True\n",
    "                    break\n",
    "            if flag:\n",
    "                break\n",
    "                \n",
    "def test():\n",
    "    print(\"Loading test data...\")\n",
    "    start_time = time.time()\n",
    "    x_test, y_test = process_datasets(test_dir, cat_to_id, conf)\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        saver = tf.train.Saver()\n",
    "        saver.restore(sess=sess, save_path=save_path)\n",
    "        \n",
    "        print(\"Testing...\")\n",
    "        loss_test, acc_test = evaluate(sess, x_test, y_test)\n",
    "        msg = 'Test Loss: {0:>6.2}, Test Acc: {1:>7.2%}'\n",
    "        print(msg.format(loss_test, acc_test))\n",
    "        \n",
    "        batch_size = config.batch_size\n",
    "        data_len = len(x_test)\n",
    "        num_batch = (data_len - 1) / batch_size\n",
    "        if (data_len - 1) % batch_size != 0:\n",
    "            num_batch += 1\n",
    "        \n",
    "        y_test_cls = np.argmax(y_test, 1)\n",
    "        y_pred_cls = np.zeros(shape=len(x_test), dtype=np.int32)\n",
    "        for i in range(num_batch):\n",
    "            start_id = i * batch_size\n",
    "            end_id = ((i + 1) * batch_size, data_len)\n",
    "            feed_dict = {\n",
    "                model.input_x: x_test[start_id:end_id],\n",
    "                model.keep_prob: 1.0\n",
    "            }\n",
    "            y_pred_cls[start_id:end_id] = sess.run(model.y_pred_cls, feed_dict=feed_dict)\n",
    "        \n",
    "        print(\"Precision, Recall and F1-Score...\")\n",
    "        print(metrics.classification_report(y_test_cls, y_pred_cls, target_names=categories))\n",
    "        \n",
    "        print(\"Confusion Matrix...\")\n",
    "        cm = metrics.confusion_matrix(y_test_cls, y_pred_cls)\n",
    "        print(cm)\n",
    "        \n",
    "        time_diff = get_time_diff(start_time)\n",
    "        print(\"Time usage: \", time_diff)\n",
    "\n",
    "        \n",
    "def run_cnn(argv):\n",
    "    if argv not in ['train', 'test']:\n",
    "        raise ValueError(\"Wrong argument.\")\n",
    "        \n",
    "    print(\"Configuring CNN Model...\")\n",
    "    config = TCNNConfig()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
