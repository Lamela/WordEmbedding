{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'objectif de ce notebook:\n",
    "1. Présenter les bases de données existantes en chinois\n",
    "2. Appliquer les méthodes de word embedding existantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import fasttext\n",
    "import time\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import pandas as pd\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 1.La présentation des jeux de données de texte existantes en chinois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons trouvé un corpus chinois. Le sujet de corpus est la nouvelle. L'origine de corpus est le toolbox [THUCTC](http://thuctc.thunlp.org/, \"THUCTC: 一个高效的中文文本分类工具\"), une boîte d'outils pour la classification de textes chinois. Ce corpus contient 14 catégories de nouvelles. Chaque nouvelle est stockée dans un fichier .txt qui se trouve dans son propre dossier de catégorie correspondant. Voici le bilan de toutes les nouvelles.\n",
    "\n",
    "| Catégorie | Nombre de nouvelles |\n",
    "| :--------: | -----------------: |\n",
    "| Sport | 131604 |\n",
    "| Divertissement | 92632 |\n",
    "| Maison | 32586 |\n",
    "| Loterie | 7588 |\n",
    "| Propriété | 20050 |\n",
    "| Éducation | 41936 |\n",
    "| Mode | 13368 |\n",
    "| Affaire | 63086 |\n",
    "| Constellation | 3578 |\n",
    "| Jeu de vidéo | 24373 |\n",
    "| Société | 50849 |\n",
    "| Science et Technologie | 162929 |\n",
    "| Action | 154398 |\n",
    "| Finance et Économie | 37098 |\n",
    "| **Totale** | **836075**|\n",
    "\n",
    "- Le corpus utilisé dans ce notebook est un corpus de nouvelles de THUCTC <https://blog.csdn.net/lxg0807/article/details/52960072>. 10000 nouvelles ont été sélectionné dans chaque catégorie et ensuite ont été mis dans un seul fichier pour l'entraînement. Pour les catégories qui ne comportent pas 10000 pièces, toutes ses nouvelles ont été sélectionnées. Le jeu de données de test suit le même principe. Cependant, il ne contient pas de catégorie qui possède moins que 10000 nouvelles parce qu'ils sont tous dans le jeu de données d'apprentissage. Le [détail du code](https://blog.csdn.net/lxg0807/article/details/52776183).\n",
    "\n",
    "- Les jeux de données utilisés dans ce notebook peuvent être téléchargés:\n",
    "\n",
    "    [news_fasttext_train.txt](https://drive.google.com/file/d/1rF8jaifMPug2QSxYTfSJuAI6ThUEsGBi/view?usp=sharing)\n",
    "\n",
    "    [news_fasttext_test.txt](https://drive.google.com/file/d/1psrAH5heISv3t2xuB4YKxUCvsUN6yU6C/view?usp=sharing)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Pour commencer, nous allons lire le jeu de données d'apprentissage et montrer quelques exemples de nouvelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英媒称 威廉 王子 圣诞节 前 将 宣布 订婚   　 　 中新网 11 月 8 日电   据 香港 《 文汇报 》 报道 ， 英媒称 英国 威廉 王子 和 女友 凯蒂 将 于 圣诞节 前 宣布 订婚 ， 并 于 明年 7 月 23 日拉埋 天窗 ， 结束 8 年 爱情 长跑 ， 并 计划 婚后 生儿育女 ， 组织 幸福家庭 。   　 　 报道 指 ， 28 岁 的 凯蒂 预定 与 王室 成员 一起 ， 在 诺福克郡 桑德灵 厄姆堡 欢度 圣诞节 ， 但 她 须 先 与 威廉 订婚 才 可 获邀 出席 。 消息人士 透露 ， 一切 安排 都 是 为 两人 明年 的 婚事 铺路 ， 而且 “ 进展 迅速 ” 。   　 　 两人 决定 在 温莎堡 乔治 教堂 低调 成婚 ， 这里 也 是 王储 查尔斯 和 卡米拉 2005 年 结婚 之 处 。 据悉 英女王 也 希望 在 政府 紧缩 开支 时 ， 避免 婚事 太 铺张浪费 。   　 　 传媒 本周 拍摄 到 凯蒂 父母 在 女王 位于 苏格兰 的 巴尔 莫 勒尔 堡 打猎 ， 更 获 女王 批准 在 附近 一间 王宫 住宿 ， 令 威廉 、 凯蒂 快 将 宣布 婚事 的 传闻 甚嚣尘上 。   　 　 另外 ， 女王 跟随 威廉 、 哈里 王子 和 其他 年轻 王室 成员 ， 正式 加入 大热 社交 网站 facebook ， 成立 王室 官方 页面 ， 公布 王室 活动 、 照片 、 影片 、 新闻 和 发言 。 facebook 用家 对 页面 点击 “ 赞 ” ( like ) ， 就 可 定期 收到 活动 更新 。   　 　 中新网 11 月 8 日电   据 香港 《 文汇报 》 报道 ， 英媒称 英国 威廉 王子 和 女友 凯蒂 将 于 圣诞节 前 宣布 订婚 ， 并 于 明年 7 月 23 日拉埋 天窗 ， 结束 8 年 爱情 长跑 ， 并 计划 婚后 生儿育女 ， 组织 幸福家庭 。   　 　 报道 指 ， 28 岁 的 凯蒂 预定 与 王室 成员 一起 ， 在 诺福克郡 桑德灵 厄姆堡 欢度 圣诞节 ， 但 她 须 先 与 威廉 订婚 才 可 获邀 出席 。 消息人士 透露 ， 一切 安排 都 是 为 两人 明年 的 婚事 铺路 ， 而且 “ 进展 迅速 ” 。   　 　 两人 决定 在 温莎堡 乔治 教堂 低调 成婚 ， 这里 也 是 王储 查尔斯 和 卡米拉 2005 年 结婚 之 处 。 据悉 英女王 也 希望 在 政府 紧缩 开支 时 ， 避免 婚事 太 铺张浪费 。   　 　 传媒 本周 拍摄 到 凯蒂 父母 在 女王 位于 苏格兰 的 巴尔 莫 勒尔 堡 打猎 ， 更 获 女王 批准 在 附近 一间 王宫 住宿 ， 令 威廉 、 凯蒂 快 将 宣布 婚事 的 传闻 甚嚣尘上 。   　 　 另外 ， 女王 跟随 威廉 、 哈里 王子 和 其他 年轻 王室 成员 ， 正式 加入 大热 社交 网站 facebook ， 成立 王室 官方 页面 ， 公布 王室 活动 、 照片 、 影片 、 新闻 和 发言 。 facebook 用家 对 页面 点击 “ 赞 ” ( like ) ， 就 可 定期 收到 活动 更新 。    \t__label__affairs\n",
      "\n",
      "张庭 林瑞阳 公开 得 女 喜讯   酒窝 小美女 招人 爱   　 　 中新网 12 月 18 日电   39 岁 的 台湾 艺人 张庭 ， 18 日 和 林瑞阳 公开 得 女 喜讯 ， 本月 10 日 已 剖腹 生下重 2910 公克 的 射手 宝宝 ， 母女均安 ， 女儿 遗传 爸爸 的 修长 身材 ， 有 双 长长的 腿 ， 脸蛋 则 像 妈妈 ， 五官 明显 ， 也 有 对 可爱 的 小 酒窝 ， 两人 先帮 女儿 取小 名叫 “ 囡囡 ” 。   　 　 张庭 为了 生小孩 ， 努力 很 久 ， 除 吃 中药 调养 ， 也 做 多次 人工受孕 。 她 在 怀孕 过程 很 小心 ， 体重 也 控制 很 好 ， 整个 孕程 才 胖 10 多公斤 。 第三度 当 爸爸 的 林瑞阳 ， 全程 陪伴 张庭 。    \t__label__ent\n",
      "\n",
      "Nombre de nouvelles: 121155\n"
     ]
    }
   ],
   "source": [
    "with open('datasets/news_fasttext_train.txt') as f:\n",
    "    f_news = f.readlines()\n",
    "    \n",
    "for i in range(2):\n",
    "    print(f_news[i*40001])\n",
    "print(\"Nombre de nouvelles: \" + str(len(f_news)))\n",
    "del(f_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Nous avons vu, chaque ligne du text contient une nouvelle. Tous les mots chinois ont été séparés par un espace. Les charactères liées correspondent un mot en chinois. À la fin de chaque ligne, une étiquette du sujet est marquée, avec préfix \"\\_\\_label__\".\n",
    "\n",
    "Le nombre totale de nouvelles dan ce jeu d'apprentissage est 121155."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 2. Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant construire le jeu de données pour entraîner des vecteurs de Word Embedding.\n",
    "- D'abord, nous allons éliminer les mots non chinois et les étiquettes. Vue que l'entraînement de vecteurs de mots est l'apprentissage non supervisé, les étiquettes ne sont pas utiles.\n",
    "- Dans ce cas là, nous allons définir une méthode pour éliminer les mots non chinois et les étiquettes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter(s):\n",
    "    \"\"\"\n",
    "    Output only Chinese characters\n",
    "    \"\"\"\n",
    "    filtrate = re.compile(u'[^\\u4E00-\\u9FA5]')\n",
    "    res = filtrate.sub(r' ', s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vue que la taille de données est assez grande, pour réduire l'utilisation de mémoire, nous allons faire le prétraitement de données ligne par ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/news_fasttext_train.txt') as origin:\n",
    "    with open('datasets/news_pure_nolabel.txt', 'w') as f:\n",
    "        for line in origin:\n",
    "            temp = filter(line).replace('  ', '')\n",
    "            f.write(\"{}\\n\".format(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons écrit les données dans un fichier, qui peut être téléchargé via [news_pure_nolabel.txt](https://drive.google.com/file/d/1raU-oYJiZDa1U2bQcUrOuwilsQ3x49XC/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 3. Entraîner le classifier de texte par FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train model: 18.09974718093872\n",
      "Time used to test model: 6.489847898483276\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "clf = fasttext.supervised('news_fasttext_train.txt', 'model')\n",
    "toc = time.time()\n",
    "print('Time used to train model: ' + str(toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "res = clf.test('news_fasttext_test.txt')\n",
    "toc = time.time()\n",
    "print('Time used to test model: ' + str(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140941674970252\n",
      "0.9140941674970252\n",
      "103369\n",
      "<fasttext.model.SupervisedModel object at 0x7f9263b791d0>\n",
      "Help on method_descriptor:\n",
      "\n",
      "classifier_test(...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.precision)\n",
    "print(res.recall)\n",
    "print(res.nexamples)\n",
    "print(clf)\n",
    "help(fasttext.fasttext.FastTextModelWrapper.classifier_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 4. Entraîner les modèles de représentation de mots par FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Il existe deux types de modèles:\n",
    "    1. Skipgram modèle\n",
    "    2. CBOW modèle\n",
    "\n",
    "Les librairies **fasttext** et **gensim** permet d'entraîner les deux modèles en utilisant FastText et Word2Vec. Nous allons commencer par Fasttext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Nous avons utilisé les paramètres par défault pour entraîner ces modèles. Quelques paramètres importants:\n",
    "\n",
    "| Paramètre | Value |\n",
    "| --------- | ----- |\n",
    "| epoch | 5 |\n",
    "| vector_size | 100 |\n",
    "| min_count | 5 |\n",
    "| window_size | 5 |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train skipgram word representation model: 487.47211360931396\n",
      "183312\n"
     ]
    }
   ],
   "source": [
    "# Modèle Skipgram \n",
    "tic = time.time()\n",
    "model_skipgram = fasttext.skipgram('datasets/news_pure_nolabel.txt', 'model_skipgram')\n",
    "toc = time.time()\n",
    "print('Time used to train skipgram word representation model: ' + str(toc - tic) + \"s\")\n",
    "\n",
    "print(\"Vocabulary size: \" + str(len(model_skipgram.words))) # list of words in dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train cbow word representation model: 180.20052647590637\n",
      "183312\n"
     ]
    }
   ],
   "source": [
    "# Modèle CBOW\n",
    "tic = time.time()\n",
    "model_cbow = fasttext.cbow('datasets/news_pure_nolabel.txt', 'model_cbow')\n",
    "toc = time.time()\n",
    "print('Time used to train cbow word representation model: ' + str(toc - tic) + \"s\")\n",
    "\n",
    "print(\"Vocabulary size: \" + str(len(model_cbow.words))) # list of words in dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 5. Entraîner le modèle de représentation de mots par Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous utilisons le module Word2Vec de package **gensim** pour faire l'apprentissage de modèle. Pareil comme FastText, nous allons entraîner Skipgram et CBOW avec les paramètres principaux comme FastText."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train a word2vec_skipgram word representation model: 520.1113519668579s\n"
     ]
    }
   ],
   "source": [
    "file_path = 'datasets/news_pure_nolabel.txt'\n",
    "\n",
    "# Modèle Skipgram\n",
    "tic = time.time()\n",
    "model_word2vec = Word2Vec(LineSentence(file_path), workers=6, sg=1)\n",
    "toc = time.time()\n",
    "print(\"Time used to train a word2vec_skipgram word representation model: \" + str(toc - tic) + \"s\")\n",
    "\n",
    "# Sauvegarder le modèle et les vecteurs entraînés\n",
    "model_word2vec.save('word2vec_skip.model')\n",
    "model_word2vec.wv.save_word2vec_format('word2vec_skip.txt',binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train a word2vec_cbow word representation model: 219.7475070953369s\n"
     ]
    }
   ],
   "source": [
    "# Modèle CBOW\n",
    "tic = time.time()\n",
    "model_word2vec = Word2Vec(LineSentence(file_path), workers=6)\n",
    "toc = time.time()\n",
    "print(\"Time used to train a word2vec_cbow word representation model: \" + str(toc - tic) + \"s\")\n",
    "\n",
    "# Sauvegarder le modèle et les vecteurs entraînés\n",
    "model_word2vec.save('word2vec_cbow.model')\n",
    "model_word2vec.wv.save_word2vec_format('word2vec_cbow.txt',binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 6. Entraîner le modèle de représentation de mots par GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'entraînement de modèle est fait depuis les code source de Stanford. Il n'existe pas de librairie python, donc nous allons exécuter les lignes de commande pour l'entraînement. Les paramètres principaux sont les même que ceux d'avant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- D'abord nous comptons le nombre d'occurrence pour chaque mot qui apparaît au minimum 5 fois et stockons les résultat dans le fichier ***news_vocab***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "BUILDING VOCABULARY\n",
      "Truncating vocabulary at min count 5.\n",
      "Using vocabulary of size 183311.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdout, stderr = subprocess.Popen(\"GloVe-1.2/build/vocab_count -min-count 5 -verbose 1 < datasets/news_pure_nolabel.txt > news_vocab\", \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()\n",
    "print(stdout.decode('utf-8'))\n",
    "print(stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ensuite nous calculons la matrice de co-occurrence des mots avec ***window_size = 5*** et stockons la matrice dans le fichier ***news_cooccur.bin***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "COUNTING COOCCURRENCES\n",
      "window size: 5\n",
      "context: symmetric\n",
      "\u001b[0GMerging cooccurrence files: processed 93438016 lines.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdout, stderr = subprocess.Popen(\"GloVe-1.2/build/cooccur -memory 4.0 -vocab-file news_vocab -verbose 1 -window-size 5 < datasets/news_pure_nolabel.txt > news_cooccur.bin\", \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()\n",
    "print(stdout.decode('utf-8'))\n",
    "print(stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous allons mélanger la matrice et le stocker dans le fichier ***news_shuffle.bin***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SHUFFLING COOCCURRENCES\n",
      "array size: 255013683\n",
      "Merging temp files: processed 0 lines.\u001b[31G93438016 lines.\u001b[0GMerging temp files: processed 93438016 lines.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "stdout, stderr = subprocess.Popen(\"GloVe-1.2/build/shuffle -memory 4.0 -verbose 1 < news_cooccur.bin > news_shuffle.bin\", \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()\n",
    "print(stdout.decode('utf-8'))\n",
    "print(stderr.decode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Maintenant nous allons entraîner le modèle GloVe avec les paramètres principaux comme d'avant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TRAINING MODEL\n",
      "Read 93438016 lines.\n",
      "vector size: 100\n",
      "vocab size: 183311\n",
      "x_max: 10.000000\n",
      "alpha: 0.750000\n",
      "iter: 001, cost: 0.092556\n",
      "iter: 002, cost: 0.068930\n",
      "iter: 003, cost: 0.059226\n",
      "iter: 004, cost: 0.053823\n",
      "iter: 005, cost: 0.050688\n",
      "\n",
      "Time used to train a glove word representation model: 261.8131902217865s\n"
     ]
    }
   ],
   "source": [
    "tic = time.time()\n",
    "stdout, stderr = subprocess.Popen(\"GloVe-1.2/build/glove -save-file glove -threads 8 -input-file news_shuffle.bin -vocab-file news_vocab -x-max 10 -iter 5 -vector-size 100 -write_header 1 -binary 2 -verbose 1\", \n",
    "                                  stdout=subprocess.PIPE, stderr=subprocess.PIPE, shell=True).communicate()\n",
    "toc = time.time()\n",
    "print(stdout.decode('utf-8'))\n",
    "print(stderr.decode('utf-8'))\n",
    "print(\"Time used to train a glove word representation model: \" + str(toc - tic) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pour que le fichier de vecteurs de mots puisse être utilisé pour l'évaluation comme les deux autre méthodes, nous allons ajouter une ligne dans ***glove.txt*** pour indiquer le nombre de mots et la taille de vecteur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of word: 183312\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(None, None)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines = 0\n",
    "glove_file = \"glove.txt\"\n",
    "with open(glove_file) as f:\n",
    "    for line in f:\n",
    "        lines += 1\n",
    "print('Total number of word: ' + str(lines))\n",
    "line = str(lines) + \" 100\"\n",
    "subprocess.Popen(\"sed -i '1 i {}' {}\".format(line, glove_file), shell=True).communicate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 7. Évaluation des vecteurs de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Après avoir entraîné le modèle, il y a trois applications de modèles usuelles:\n",
    "    1. Chercher un ensemble de mots qui resemble à un mot donné\n",
    "    2. Calculer le niveau de similarité de deux mots\n",
    "    3. Déterminer le mot qui est différent que les autres mots dans un ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "女朋友 0.7221945524215698\n",
      "男朋友 0.679075300693512\n",
      "儿媳妇 0.674573540687561\n",
      "老人家 0.6708532571792603\n",
      "狄波拉 0.6495078802108765\n"
     ]
    }
   ],
   "source": [
    "req_count = 5\n",
    "for key in model_word2vec.wv.similar_by_word('老婆', topn =100):\n",
    "    if len(key[0])==3:\n",
    "        req_count -= 1\n",
    "        print(key[0], key[1])\n",
    "        if req_count == 0:\n",
    "            break;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cette partie est de chercher 5 mots qui resemblent le plus à un mot donné"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le niveau de similarité de deux mots: 0.6405021458689147\n",
      "le niveau de similarité de deux mots: 0.8646403886585726\n",
      "le niveau de similarité de deux mots: 0.3419415565970098\n"
     ]
    }
   ],
   "source": [
    "print(\"le niveau de similarité de deux mots: \" + str(model_word2vec.wv.similarity('石头', '沙子')))\n",
    "print(\"le niveau de similarité de deux mots: \" + str(model_word2vec.wv.similarity('深圳', '广州')))\n",
    "print(\"le niveau de similarité de deux mots: \" + str(model_word2vec.wv.similarity('深圳', '深圳大学')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cette partie est de calculer le niveau de similarité de deux mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国\n"
     ]
    }
   ],
   "source": [
    "print(model_word2vec.wv.doesnt_match(u\"英国 法国 西班牙 意大利 中国\".split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- cette partie est de déterminer le mot qui est différent que les autres mots dans un ensemble"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
