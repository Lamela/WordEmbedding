{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.La présentation des jeux de données de texte existantes en chinois"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons trouvé un corpus chinois. Le sujet de corpus est la nouvelle. L'origine de corpus est le toolbox [THUCTC](http://thuctc.thunlp.org/, \"THUCTC: 一个高效的中文文本分类工具\"), une boîte à outils pour la classification des textes chinois. Ce corpus contient 14 catégories de nouvelle. Chaque pièce de nouvelle est stockée dans un fichier .txt qui se trouve dans sa propre dossier de catégorie correspondante. Voici le bilan de tout les nouvelles.\n",
    "\n",
    "| Catégorie | Nombre de nouvelle |\n",
    "| --------- | ------------------ |\n",
    "| Sport | 131604 |\n",
    "| Divertissement | 92632 |\n",
    "| Maison | 32586 |\n",
    "| Loterie | 7588 |\n",
    "| Propriété | 20050 |\n",
    "| Éducation | 41936 |\n",
    "| Mode | 13368 |\n",
    "| Affaire | 63086 |\n",
    "| Constellation | 3578 |\n",
    "| Jeu de vidéo | 24373 |\n",
    "| Société | 50849 |\n",
    "| Science et Technologie | 162929 |\n",
    "| Action | 154398 |\n",
    "| Finance et Économie | 37098 |\n",
    "| **Totale** | **836075**|\n",
    "\n",
    "- Le corpus utilisé dans ce notebook sont des nouvelles sélectionnées depuis THUCTC corpus <https://blog.csdn.net/lxg0807/article/details/52960072>. 10000 pièces de nouvelles sont sélectionnées dans chaque catégorie et ensuite sont mis dans un seul fichier pour le jeu d'apprentissage. Pour les catégories qui ne comporte pas 10000 pièces, tout les nouvelles sont sélectionnées. Le jeu de test suit le même principe. Cependant, il ne contient pas de catégorie de moins de 10000 pièces parce que ils sont tout sélectionnées par le jeu d'apprentissage. Le [détail du code](https://blog.csdn.net/lxg0807/article/details/52776183).\n",
    "\n",
    "- Les données utilisées dans ce notebook peuvent être téléchargées:\n",
    "\n",
    "    [news_fasttext_train.txt](https://drive.google.com/file/d/1rF8jaifMPug2QSxYTfSJuAI6ThUEsGBi/view?usp=sharing)\n",
    "\n",
    "    [news_fasttext_test.txt](https://drive.google.com/file/d/1psrAH5heISv3t2xuB4YKxUCvsUN6yU6C/view?usp=sharing)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------\n",
    "Pour commencer, nous allons lire le jeu d'apprentissage et montrer quelques exemples de nouvelles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "英媒称 威廉 王子 圣诞节 前 将 宣布 订婚   　 　 中新网 11 月 8 日电   据 香港 《 文汇报 》 报道 ， 英媒称 英国 威廉 王子 和 女友 凯蒂 将 于 圣诞节 前 宣布 订婚 ， 并 于 明年 7 月 23 日拉埋 天窗 ， 结束 8 年 爱情 长跑 ， 并 计划 婚后 生儿育女 ， 组织 幸福家庭 。   　 　 报道 指 ， 28 岁 的 凯蒂 预定 与 王室 成员 一起 ， 在 诺福克郡 桑德灵 厄姆堡 欢度 圣诞节 ， 但 她 须 先 与 威廉 订婚 才 可 获邀 出席 。 消息人士 透露 ， 一切 安排 都 是 为 两人 明年 的 婚事 铺路 ， 而且 “ 进展 迅速 ” 。   　 　 两人 决定 在 温莎堡 乔治 教堂 低调 成婚 ， 这里 也 是 王储 查尔斯 和 卡米拉 2005 年 结婚 之 处 。 据悉 英女王 也 希望 在 政府 紧缩 开支 时 ， 避免 婚事 太 铺张浪费 。   　 　 传媒 本周 拍摄 到 凯蒂 父母 在 女王 位于 苏格兰 的 巴尔 莫 勒尔 堡 打猎 ， 更 获 女王 批准 在 附近 一间 王宫 住宿 ， 令 威廉 、 凯蒂 快 将 宣布 婚事 的 传闻 甚嚣尘上 。   　 　 另外 ， 女王 跟随 威廉 、 哈里 王子 和 其他 年轻 王室 成员 ， 正式 加入 大热 社交 网站 facebook ， 成立 王室 官方 页面 ， 公布 王室 活动 、 照片 、 影片 、 新闻 和 发言 。 facebook 用家 对 页面 点击 “ 赞 ” ( like ) ， 就 可 定期 收到 活动 更新 。   　 　 中新网 11 月 8 日电   据 香港 《 文汇报 》 报道 ， 英媒称 英国 威廉 王子 和 女友 凯蒂 将 于 圣诞节 前 宣布 订婚 ， 并 于 明年 7 月 23 日拉埋 天窗 ， 结束 8 年 爱情 长跑 ， 并 计划 婚后 生儿育女 ， 组织 幸福家庭 。   　 　 报道 指 ， 28 岁 的 凯蒂 预定 与 王室 成员 一起 ， 在 诺福克郡 桑德灵 厄姆堡 欢度 圣诞节 ， 但 她 须 先 与 威廉 订婚 才 可 获邀 出席 。 消息人士 透露 ， 一切 安排 都 是 为 两人 明年 的 婚事 铺路 ， 而且 “ 进展 迅速 ” 。   　 　 两人 决定 在 温莎堡 乔治 教堂 低调 成婚 ， 这里 也 是 王储 查尔斯 和 卡米拉 2005 年 结婚 之 处 。 据悉 英女王 也 希望 在 政府 紧缩 开支 时 ， 避免 婚事 太 铺张浪费 。   　 　 传媒 本周 拍摄 到 凯蒂 父母 在 女王 位于 苏格兰 的 巴尔 莫 勒尔 堡 打猎 ， 更 获 女王 批准 在 附近 一间 王宫 住宿 ， 令 威廉 、 凯蒂 快 将 宣布 婚事 的 传闻 甚嚣尘上 。   　 　 另外 ， 女王 跟随 威廉 、 哈里 王子 和 其他 年轻 王室 成员 ， 正式 加入 大热 社交 网站 facebook ， 成立 王室 官方 页面 ， 公布 王室 活动 、 照片 、 影片 、 新闻 和 发言 。 facebook 用家 对 页面 点击 “ 赞 ” ( like ) ， 就 可 定期 收到 活动 更新 。    \t__label__affairs\n",
      "\n",
      "张庭 林瑞阳 公开 得 女 喜讯   酒窝 小美女 招人 爱   　 　 中新网 12 月 18 日电   39 岁 的 台湾 艺人 张庭 ， 18 日 和 林瑞阳 公开 得 女 喜讯 ， 本月 10 日 已 剖腹 生下重 2910 公克 的 射手 宝宝 ， 母女均安 ， 女儿 遗传 爸爸 的 修长 身材 ， 有 双 长长的 腿 ， 脸蛋 则 像 妈妈 ， 五官 明显 ， 也 有 对 可爱 的 小 酒窝 ， 两人 先帮 女儿 取小 名叫 “ 囡囡 ” 。   　 　 张庭 为了 生小孩 ， 努力 很 久 ， 除 吃 中药 调养 ， 也 做 多次 人工受孕 。 她 在 怀孕 过程 很 小心 ， 体重 也 控制 很 好 ， 整个 孕程 才 胖 10 多公斤 。 第三度 当 爸爸 的 林瑞阳 ， 全程 陪伴 张庭 。    \t__label__ent\n",
      "\n",
      "Total length: 121155\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "with open('datasets/news_fasttext_train.txt') as f:\n",
    "    f_news = f.readlines()\n",
    "    \n",
    "for i in range(2):\n",
    "    print(f_news[i*40001])\n",
    "print(\"Total length: \" + str(len(f_news)))\n",
    "del(f_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "Nous avons vu, dans le text, chaque ligne du text contient une nouvelle. Tous les mots chinois ont été séparés par un espace. Les charactères liées sont des mots en chinois. À la fin de chaque ligne, une étiquette du sujet est marquée, avec préfix \"\\_\\_label__\".\n",
    "\n",
    "Le nombre totale de nouvelles dan ce jeu d'apprentissage est 121155."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 2. Prétraitement des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons maintenant construire le jeu de données pour entraîner des vecteurs de Word Embedding.\n",
    "- D'abord, nous allons éliminer les mots non chinois et les labels. Vue que l'entraînement des vecteurs de mots est l'apprentissage non supervisée, les labels ne sont donc pas utile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def filter(s):\n",
    "    \"\"\"\n",
    "    Output only Chinese characters\n",
    "    \"\"\"\n",
    "    filtrate = re.compile(u'[^\\u4E00-\\u9FA5]')\n",
    "    res = filtrate.sub(r' ', s)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vue que les données sont assez grandes, pour réduire l'utilisation de mémoire, le prétraitement le l'écriture seront faits ligne par ligne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets/news_fasttext_train.txt') as origin:\n",
    "    with open('news_pure_nolabel.txt', 'w') as f:\n",
    "        for line in origin:\n",
    "            temp = filter(line).replace('  ', '')\n",
    "            f.write(\"{}\\n\".format(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Nous avons écrit les données dans un fichier, qui peut être téléchargé via [news_pure_nolabel.txt](https://drive.google.com/file/d/1raU-oYJiZDa1U2bQcUrOuwilsQ3x49XC/view?usp=sharing)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "# 3. Entraîner le modèle FastText\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train model: 18.09974718093872\n",
      "Time used to test model: 6.489847898483276\n"
     ]
    }
   ],
   "source": [
    "import fasttext\n",
    "import time\n",
    "# Skipgram model\n",
    "tic = time.time()\n",
    "clf = fasttext.supervised('news_fasttext_train.txt', 'model')\n",
    "toc = time.time()\n",
    "print('Time used to train model: ' + str(toc - tic))\n",
    "\n",
    "tic = time.time()\n",
    "res = clf.test('news_fasttext_test.txt')\n",
    "toc = time.time()\n",
    "print('Time used to test model: ' + str(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9140941674970252\n",
      "0.9140941674970252\n",
      "103369\n",
      "<fasttext.model.SupervisedModel object at 0x7f9263b791d0>\n",
      "Help on method_descriptor:\n",
      "\n",
      "classifier_test(...)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(res.precision)\n",
    "print(res.recall)\n",
    "print(res.nexamples)\n",
    "print(clf)\n",
    "help(fasttext.fasttext.FastTextModelWrapper.classifier_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tic = time.time()\n",
    "model = fasttext.cbow('news_fasttext_train.txt', 'model')\n",
    "toc = time.time()\n",
    "print('Time used to train cbow word embedding: ' + str(toc - tic))\n",
    "\n",
    "print(len(model.words)) # list of words in dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "[-0.05577888339757919, 0.5747898817062378, 0.22692202031612396, 0.4946122169494629, -0.18957051634788513, -0.17648553848266602, -0.39239057898521423, -0.0555281937122345, 0.18335533142089844, 0.11815828084945679, 0.56656813621521, -0.22199392318725586, 0.5065208673477173, 0.40128207206726074, 0.43143612146377563, 0.4246213436126709, -0.277787446975708, -0.22947901487350464, 0.45027220249176025, 0.1363823115825653, -0.2217646837234497, 0.192704975605011, -0.09822869300842285, 0.006437172647565603, 0.1444288194179535, 0.27505701780319214, -0.3348937928676605, -0.023767095059156418, -0.2854846119880676, -0.8086239099502563, -0.09918409585952759, -0.029884761199355125, 0.1913653463125229, -0.2349591702222824, -0.3739858567714691, 0.0973408967256546, 0.31533583998680115, -0.4581424593925476, -0.2626631557941437, 0.11524378508329391, 0.3455769419670105, -0.028281312435865402, -0.29140642285346985, -0.21107324957847595, 0.47397279739379883, -0.5091606974601746, -0.9433766603469849, 0.7063651084899902, -0.6494448184967041, -0.4895438849925995, 0.5994689464569092, 0.11583619564771652, -0.0841178447008133, -0.3619289994239807, -0.20883285999298096, 0.2988636791706085, -0.08405117690563202, 0.11775737255811691, 0.6176376342773438, 0.16744780540466309, 0.3242085874080658, 0.27183616161346436, 0.0873773917555809, 0.7539441585540771, 0.12515044212341309, 0.054456811398267746, -0.42738014459609985, 0.20416554808616638, 0.17918089032173157, -0.13627362251281738, -0.3536536395549774, -0.4297832250595093, -0.29717183113098145, -0.3300507068634033, 0.3676673173904419, -0.2739403247833252, -0.6043366193771362, -0.42655473947525024, -0.17440664768218994, -0.3325427770614624, -0.33102184534072876, 0.3008868992328644, -0.06647499650716782, 0.13894984126091003, -0.011803041212260723, 0.3114173710346222, 0.19143395125865936, 0.47240447998046875, 0.0121158417314291, 0.17692407965660095, -0.27465081214904785, -0.11435313522815704, 0.9519168734550476, -0.15112298727035522, 0.6985753774642944, -0.27682527899742126, 0.5423479080200195, -0.1768427938222885, 0.3599781394004822, 0.1936415135860443]\n"
     ]
    }
   ],
   "source": [
    "print('大英国' in model.words)\n",
    "print(model['大英国'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.634902570280985\n",
      "20.134612434390935\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.linalg.norm((model['中国'], model['北京'])))\n",
    "\n",
    "print(np.linalg.norm((model['德国'], model['伦敦'])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used to train a word2vec model: 837.5510361194611\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import multiprocessing\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.word2vec import LineSentence\n",
    "import time\n",
    "\n",
    "file_path = 'news_pure_nolabel.txt'\n",
    "tic = time.time()\n",
    "model = Word2Vec(LineSentence(file_path), workers=6, min_count=3, iter=20)\n",
    "toc = time.time()\n",
    "print(\"Time used to train a word2vec model: \" + str(toc - tic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('word2vec_cbow.model')\n",
    "model.wv.save_word2vec_format('word2vec_cbow.txt',binary = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
