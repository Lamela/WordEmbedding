{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, LSTM\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_util import plot_confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "# Model parameters\n",
    "MAX_FEATURES = 256404\n",
    "MAX_TEXT_LENGTH = 1024\n",
    "EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons la fonction pour lire le jeu de données et faire le découpage d'apprentissage et test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(data_paths, test_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for data_path in data_paths:\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                temp = line.split('__label__')\n",
    "                x.append(temp[0])\n",
    "                y.append(temp[1].replace('\\n', ''))\n",
    "            \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "    print('Dataset splited.')\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour construire le pipeline, nous allons d'abord définir des fonctions pour pré-traiter les données.\n",
    "\n",
    "Nous allons tokenizer les données et nous mettrons les données et les labels au format propre pour le réseau de neurones. Les labels seront transmis aux vecteurs one-hot. Les données seront tokenizées et mises à la même longueur. Nous coupons les exemples trop longs et nous utilisons zero-padding pour les exemples courts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    word_index = tokenizer.word_index\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           word_index\n",
    "\n",
    "\n",
    "def class_str_2_ind(x_train, x_test, y_train, y_test):\n",
    "    print('Converting data to trainable form...')\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    n_out = len(CLASSES_LIST)\n",
    "    le.fit(CLASSES_LIST)\n",
    "    y_train = le.transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "    x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test)\n",
    "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
    "    print('Number of training examples: ' + str(len(x_vec_test)))\n",
    "    \n",
    "    return x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons la fonction pour lire les vecteurs de mots pré-appris. Nous construissons aussi la matrice d'entrée depuis x_train et les vecteurs de mots. Cette partie est basé sur le [tutoriel de Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(vectors_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(vectors_file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        #print(first_line)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_embedding_matrix(word_index, embedding_index):\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('Embedding matrix built.')        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons un modèle de classification de texte avec une couche de Conv1D et une couche de Dense. Nous pouvons utiliser les vecteurs de mots pré-appris. Le modèle est basé sur le [TP de M. Kermorvant](https://gitlab.com/kermorvant/nlp-labs) et https://github.com/gaussic/text-classification-cnn-rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, word_index, print_sum=True):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "\n",
    "    model = Embedding(len(word_index) + 1,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    \n",
    "    #model = Dropout(0.2)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    #model = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(model)\n",
    "    #model = Dropout(0.5)(model)\n",
    "    #model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(512, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(11, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_sum:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y_train):\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1)\n",
    "\n",
    "    return model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous plottons la matrice de confusion, code basé sur [l'exemple de sklearn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, y_predicted):\n",
    "    conf_mat = confusion_matrix(y_test, y_predicted)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    plot_confusion_matrix(conf_mat, CLASSES_LIST, title='Confusion matrix, without normalization')\n",
    "    plt.subplot(122)\n",
    "    plot_confusion_matrix(conf_mat, CLASSES_LIST, normalize=True, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous construissons le pipeline pour evaluer la qualité des vecteurs de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(vectors_file_path, print_sum=True, plot_mat=True):\n",
    "    embedding_index = get_embedding_index(vectors_file_path)\n",
    "    embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "    print('Building model...')\n",
    "    model = get_model(embedding_matrix, word_index, print_sum=print_sum)\n",
    "    y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat).argmax(1)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    if plot_mat:\n",
    "        plot_conf_mat(y_test, y_predicted)\n",
    "    return accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons lire le jeu de données *[news_fastexttext_test.txt](https://drive.google.com/open?id=1psrAH5heISv3t2xuB4YKxUCvsUN6yU6C)* et *[news_less_category.txt](https://drive.google.com/open?id=11kUgD4HhqRqhEhS_6yTx_QOXKCWGF3U8)*, puis faire le découpage afin d'obtenir le jeu de données d'apprentissage utilisé et le jeu de données de test utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splited.\n",
      "Label categories: ['affairs' 'economic' 'edu' 'ent' 'fashion' 'game' 'home' 'house'\n",
      " 'science' 'sports' 'stock']\n",
      "Converting data to trainable form...\n",
      "Number of training examples: 192022\n",
      "Number of training examples: 21336\n"
     ]
    }
   ],
   "source": [
    "data_paths = ['datasets/news_less_category.txt', 'datasets/news_fasttext_test.txt']\n",
    "test_size = 0.1\n",
    "x_train, x_test, y_train, y_test = split_datasets(data_paths, test_size)\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "print('Label categories: ' + str(CLASSES_LIST))\n",
    "x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index = class_str_2_ind(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, nous faisons un exemple d'évaluations en utilisant *word2vec_skip*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256403 word vectors.\n",
      "Building embedding matrix...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100) into shape (300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9827e836dd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectors_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pre_trained_vectors/word2vec_skip.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8c78dae47e6b>\u001b[0m in \u001b[0;36mget_embedding_matrix\u001b[0;34m(word_index, embedding_index)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embedding matrix built.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100) into shape (300)"
     ]
    }
   ],
   "source": [
    "vectors_file_path = 'pre_trained_vectors/word2vec_skip.txt'\n",
    "embedding_index = get_embedding_index(vectors_file_path)\n",
    "embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "print('Building model...')\n",
    "model = get_model(embedding_matrix, word_index)\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat).argmax(1)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "plot_conf_mat(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons évaluer la performance de chaque méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 190726 word vectors.\n",
      "Building embedding matrix...\n",
      "Embedding matrix built.\n",
      "Building model...\n",
      "WARNING:tensorflow:From /home/chenxin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Epoch 1/2\n",
      "192022/192022 [==============================] - 26s 133us/step - loss: 0.3323 - acc: 0.8996\n",
      "Epoch 2/2\n",
      "192022/192022 [==============================] - 25s 129us/step - loss: 0.1748 - acc: 0.9481\n",
      "Test Accuracy: 0.9520059992500938\n",
      "Found 190726 word vectors.\n",
      "Building embedding matrix...\n",
      "Embedding matrix built.\n",
      "Building model...\n",
      "Epoch 1/2\n",
      "192022/192022 [==============================] - 25s 132us/step - loss: 0.4175 - acc: 0.8845\n",
      "Epoch 2/2\n",
      "192022/192022 [==============================] - 25s 130us/step - loss: 0.2316 - acc: 0.9332\n",
      "Test Accuracy: 0.9390232470941132\n",
      "\n",
      "========================================================\n",
      "Test Accuracy of raw_300: 0.939023\n"
     ]
    }
   ],
   "source": [
    "# methods = ['pre_trained_vectors/fasttext_skip.vec',\n",
    "#            'pre_trained_vectors/fasttext_cbow.vec',\n",
    "#            'pre_trained_vectors/word2vec_skip.txt',\n",
    "#            'pre_trained_vectors/word2vec_cbow.txt',\n",
    "#            'pre_trained_vectors/glove.txt']\n",
    "methods = ['pre_trained_vectors/raw_300/ft_raw_skipgram_300.vec',\n",
    "           'pre_trained_vectors/raw_300/ft_raw_cbow_300.vec']\n",
    "accuracies = {}\n",
    "for vector_file in methods:\n",
    "    acc = evaluation(vector_file, print_sum=False, plot_mat=False)\n",
    "    method = vector_file.split('/')[1].split('.')[0]\n",
    "    accuracies[method] = acc\n",
    "print()\n",
    "print('========================================================')\n",
    "for meth, acc in accuracies.items():\n",
    "    print('Test Accuracy of %s: %f' % (meth, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats nous ont montrés que la méthode SkipGram a eu une meilleur performance, avec n'import quelle méthode. Au contraire, CBOW n'est pas assez performant. GloVe est presque pareil que CBOWs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "    \n",
    "from bilm_tf.bilm import TokenBatcher, BidirectionalLanguageModel, \\\n",
    "weight_layers, dump_token_embeddings\n",
    "\n",
    "vocab_file = '/home/chenxin/ELMo/raw_5_vocab.txt'\n",
    "options_file = '/home/chenxin/ELMo/new_checkpoint/options.json'\n",
    "weight_file = '/home/chenxin/ELMo/new_checkpoint/weights.hdf5'\n",
    "#token_embedding_file = '/home/chenxin/WordEmbedding/pre_trained_vectors/raw_300/ft_raw_skipgram_300.vec'\n",
    "with open(options_file, 'r') as fin:\n",
    "    options = json.load(fin)\n",
    "\n",
    "## Now we can do inference.\n",
    "# Create a TokenBatcher to map text to token ids.\n",
    "batcher = TokenBatcher(vocab_file)\n",
    "\n",
    "# Input placeholders to the biLM.\n",
    "context_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "\n",
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(\n",
    "    options_file,\n",
    "    weight_file,\n",
    "    use_character_inputs=False,\n",
    "    #embedding_weight_file=token_embedding_file\n",
    ")\n",
    "\n",
    "# Get ops to compute the LM embeddings.\n",
    "context_embeddings_op = bilm(context_token_ids)\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    "with tf.variable_scope('', reuse=True):\n",
    "    # the reuse=True scope reuses weights from the context for the question\n",
    "    elmo_question_input = weight_layers(\n",
    "        'input', question_embeddings_op, l2_coef=0.0\n",
    "    )\n",
    "    \n",
    "elmo_context_output = weight_layers(\n",
    "    'output', context_embeddings_op, l2_coef=0.0\n",
    ")\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    # It is necessary to initialize variables once before running inference.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create batches of data.\n",
    "    context_ids = batcher.batch_sentences(tokenized_context)\n",
    "\n",
    "    # Compute ELMo representations (here for the input only, for simplicity).\n",
    "    elmo_context_input_, elmo_question_input_ = sess.run(\n",
    "        [elmo_context_input['weighted_op']],\n",
    "        feed_dict={context_token_ids: context_ids}\n",
    "    )\n",
    "\n",
    "print(elmo_context_input_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
