{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "import os\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import GRU, Dropout, MaxPooling1D, Conv1D, Flatten, LSTM\n",
    "from keras.models import Model\n",
    "import numpy as np\n",
    "import itertools\n",
    "from keras.utils import np_utils\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support, accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from keras.preprocessing import text, sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from plot_util import plot_confusion_matrix\n",
    "%matplotlib inline\n",
    "\n",
    "# Model parameters\n",
    "MAX_FEATURES = 256404\n",
    "MAX_TEXT_LENGTH = 1024\n",
    "EMBED_SIZE  = 300\n",
    "BATCH_SIZE = 128\n",
    "EPOCHS = 2\n",
    "VALIDATION_SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons la fonction pour lire le jeu de données et faire le découpage d'apprentissage et test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_datasets(data_paths, test_size):\n",
    "    x = []\n",
    "    y = []\n",
    "    for data_path in data_paths:\n",
    "        with open(data_path, 'r') as f:\n",
    "            for line in f:\n",
    "                temp = line.split('__label__')\n",
    "                x.append(temp[0])\n",
    "                y.append(temp[1].replace('\\n', ''))\n",
    "            \n",
    "    x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=test_size)\n",
    "    print('Dataset splited.')\n",
    "    return x_train, x_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour construire le pipeline, nous allons d'abord définir des fonctions pour pré-traiter les données.\n",
    "\n",
    "Nous allons tokenizer les données et nous mettrons les données et les labels au format propre pour le réseau de neurones. Les labels seront transmis aux vecteurs one-hot. Les données seront tokenizées et mises à la même longueur. Nous coupons les exemples trop longs et nous utilisons zero-padding pour les exemples courts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_test(train_raw_text, test_raw_text):\n",
    "    \n",
    "    tokenizer = text.Tokenizer(num_words=MAX_FEATURES)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(train_raw_text))\n",
    "    word_index = tokenizer.word_index\n",
    "    train_tokenized = tokenizer.texts_to_sequences(train_raw_text)\n",
    "    test_tokenized = tokenizer.texts_to_sequences(test_raw_text)\n",
    "    return sequence.pad_sequences(train_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           sequence.pad_sequences(test_tokenized, maxlen=MAX_TEXT_LENGTH), \\\n",
    "           word_index\n",
    "\n",
    "\n",
    "def class_str_2_ind(x_train, x_test, y_train, y_test):\n",
    "    print('Converting data to trainable form...')\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    n_out = len(CLASSES_LIST)\n",
    "    le.fit(CLASSES_LIST)\n",
    "    y_train = le.transform(y_train)\n",
    "    y_test = le.transform(y_test)\n",
    "    train_y_cat = np_utils.to_categorical(y_train, n_out)\n",
    "    x_vec_train, x_vec_test, word_index = get_train_test(x_train, x_test)\n",
    "    print('Number of training examples: ' + str(len(x_vec_train)))\n",
    "    print('Number of training examples: ' + str(len(x_vec_test)))\n",
    "    \n",
    "    return x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons la fonction pour lire les vecteurs de mots pré-appris. Nous construissons aussi la matrice d'entrée depuis x_train et les vecteurs de mots. Cette partie est basé sur le [tutoriel de Keras](https://blog.keras.io/using-pre-trained-word-embeddings-in-a-keras-model.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding_index(vectors_file_path):\n",
    "    embeddings_index = {}\n",
    "    with open(vectors_file_path, 'r') as f:\n",
    "        first_line = f.readline()\n",
    "        #print(first_line)\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            coefs = np.asarray(values[1:], dtype='float32')\n",
    "            embeddings_index[word] = coefs\n",
    "            \n",
    "    print('Found %s word vectors.' % len(embeddings_index))\n",
    "    \n",
    "    return embeddings_index\n",
    "\n",
    "\n",
    "def get_embedding_matrix(word_index, embedding_index):\n",
    "    print('Building embedding matrix...')\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "    print('Embedding matrix built.')        \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous définissons un modèle de classification de texte avec une couche de Conv1D et une couche de Dense. Nous pouvons utiliser les vecteurs de mots pré-appris. Le modèle est basé sur le [TP de M. Kermorvant](https://gitlab.com/kermorvant/nlp-labs) et https://github.com/gaussic/text-classification-cnn-rnn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(embedding_matrix, word_index, print_sum=True):\n",
    "\n",
    "    inp = Input(shape=(MAX_TEXT_LENGTH,))\n",
    "\n",
    "    model = Embedding(len(word_index) + 1,\n",
    "                      EMBED_SIZE,\n",
    "                      weights=[embedding_matrix],\n",
    "                      input_length=MAX_TEXT_LENGTH,\n",
    "                      trainable=False)(inp)\n",
    "    \n",
    "    #model = Dropout(0.2)(model)\n",
    "    model = Conv1D(filters=32, kernel_size=5, padding='same', activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = MaxPooling1D(pool_size=2)(model)\n",
    "    #model = Conv1D(filters=32, kernel_size=3, padding='same', activation='relu')(model)\n",
    "    #model = Dropout(0.5)(model)\n",
    "    #model = MaxPooling1D(pool_size=2)(model)\n",
    "    model = Flatten()(model)\n",
    "    model = Dense(512, activation='relu')(model)\n",
    "    model = Dropout(0.5)(model)\n",
    "    model = Dense(11, activation=\"softmax\")(model)\n",
    "    model = Model(inputs=inp, outputs=model)\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    if print_sum:\n",
    "        model.summary()\n",
    "    return model\n",
    "\n",
    "\n",
    "def train_fit_predict(model, x_train, x_test, y_train):\n",
    "    \n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              epochs=EPOCHS, verbose=1)\n",
    "\n",
    "    return model.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous plottons la matrice de confusion, code basé sur [l'exemple de sklearn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_conf_mat(y_test, y_predicted):\n",
    "    conf_mat = confusion_matrix(y_test, y_predicted)\n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.subplot(121)\n",
    "    plot_confusion_matrix(conf_mat, CLASSES_LIST, title='Confusion matrix, without normalization')\n",
    "    plt.subplot(122)\n",
    "    plot_confusion_matrix(conf_mat, CLASSES_LIST, normalize=True, title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous construissons le pipeline pour evaluer la qualité des vecteurs de mots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(vectors_file_path, print_sum=True, plot_mat=True):\n",
    "    embedding_index = get_embedding_index(vectors_file_path)\n",
    "    embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "    print('Building model...')\n",
    "    model = get_model(embedding_matrix, word_index, print_sum=print_sum)\n",
    "    y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat).argmax(1)\n",
    "    print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "    if plot_mat:\n",
    "        plot_conf_mat(y_test, y_predicted)\n",
    "    return accuracy_score(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant, nous allons lire le jeu de données *[news_fastexttext_test.txt](https://drive.google.com/open?id=1psrAH5heISv3t2xuB4YKxUCvsUN6yU6C)* et *[news_less_category.txt](https://drive.google.com/open?id=11kUgD4HhqRqhEhS_6yTx_QOXKCWGF3U8)*, puis faire le découpage afin d'obtenir le jeu de données d'apprentissage utilisé et le jeu de données de test utilisé."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset splited.\n",
      "Label categories: ['affairs' 'economic' 'edu' 'ent' 'fashion' 'game' 'home' 'house'\n",
      " 'science' 'sports' 'stock']\n",
      "Converting data to trainable form...\n",
      "Number of training examples: 192022\n",
      "Number of training examples: 21336\n"
     ]
    }
   ],
   "source": [
    "data_paths = ['datasets/news_less_category.txt', 'datasets/news_fasttext_test.txt']\n",
    "test_size = 0.1\n",
    "x_train, x_test, y_train, y_test = split_datasets(data_paths, test_size)\n",
    "CLASSES_LIST = np.unique(y_train)\n",
    "print('Label categories: ' + str(CLASSES_LIST))\n",
    "x_vec_train, x_vec_test, y_train, y_test, train_y_cat, word_index = class_str_2_ind(x_train, x_test, y_train, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "D'abord, nous faisons un exemple d'évaluations en utilisant *word2vec_skip*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 256403 word vectors.\n",
      "Building embedding matrix...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not broadcast input array from shape (100) into shape (300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-9827e836dd80>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectors_file_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'pre_trained_vectors/word2vec_skip.txt'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0membedding_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectors_file_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0membedding_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_embedding_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Building model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding_matrix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-8c78dae47e6b>\u001b[0m in \u001b[0;36mget_embedding_matrix\u001b[0;34m(word_index, embedding_index)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_vector\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0;31m# words not found in embedding index will be all-zeros.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m             \u001b[0membedding_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedding_vector\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Embedding matrix built.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0membedding_matrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: could not broadcast input array from shape (100) into shape (300)"
     ]
    }
   ],
   "source": [
    "vectors_file_path = 'pre_trained_vectors/word2vec_skip.txt'\n",
    "embedding_index = get_embedding_index(vectors_file_path)\n",
    "embedding_matrix = get_embedding_matrix(word_index, embedding_index)\n",
    "print('Building model...')\n",
    "model = get_model(embedding_matrix, word_index)\n",
    "y_predicted = train_fit_predict(model, x_vec_train, x_vec_test, train_y_cat).argmax(1)\n",
    "print(\"Test Accuracy:\", accuracy_score(y_test, y_predicted))\n",
    "plot_conf_mat(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nous allons évaluer la performance de chaque méthode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 190726 word vectors.\n",
      "Building embedding matrix...\n",
      "Embedding matrix built.\n",
      "Building model...\n",
      "WARNING:tensorflow:From /home/chenxin/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:497: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "Epoch 1/2\n",
      "192022/192022 [==============================] - 26s 133us/step - loss: 0.3323 - acc: 0.8996\n",
      "Epoch 2/2\n",
      "192022/192022 [==============================] - 25s 129us/step - loss: 0.1748 - acc: 0.9481\n",
      "Test Accuracy: 0.9520059992500938\n",
      "Found 190726 word vectors.\n",
      "Building embedding matrix...\n",
      "Embedding matrix built.\n",
      "Building model...\n",
      "Epoch 1/2\n",
      "192022/192022 [==============================] - 25s 132us/step - loss: 0.4175 - acc: 0.8845\n",
      "Epoch 2/2\n",
      "192022/192022 [==============================] - 25s 130us/step - loss: 0.2316 - acc: 0.9332\n",
      "Test Accuracy: 0.9390232470941132\n",
      "\n",
      "========================================================\n",
      "Test Accuracy of raw_300: 0.939023\n"
     ]
    }
   ],
   "source": [
    "# methods = ['pre_trained_vectors/fasttext_skip.vec',\n",
    "#            'pre_trained_vectors/fasttext_cbow.vec',\n",
    "#            'pre_trained_vectors/word2vec_skip.txt',\n",
    "#            'pre_trained_vectors/word2vec_cbow.txt',\n",
    "#            'pre_trained_vectors/glove.txt']\n",
    "methods = ['pre_trained_vectors/raw_300/ft_raw_skipgram_300.vec',\n",
    "           'pre_trained_vectors/raw_300/ft_raw_cbow_300.vec']\n",
    "accuracies = {}\n",
    "for vector_file in methods:\n",
    "    acc = evaluation(vector_file, print_sum=False, plot_mat=False)\n",
    "    method = vector_file.split('/')[1].split('.')[0]\n",
    "    accuracies[method] = acc\n",
    "print()\n",
    "print('========================================================')\n",
    "for meth, acc in accuracies.items():\n",
    "    print('Test Accuracy of %s: %f' % (meth, acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les résultats nous ont montrés que la méthode SkipGram a eu une meilleur performance, avec n'import quelle méthode. Au contraire, CBOW n'est pas assez performant. GloVe est presque pareil que CBOWs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Invalid shape initializing embedding, got [182714, 512], expected (182714, 300)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-83562261a714>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;31m# Get ops to compute the LM embeddings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m \u001b[0mcontext_embeddings_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbilm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext_token_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConfigProto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mallow_soft_placement\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, ids_placeholder)\u001b[0m\n\u001b[1;32m     95\u001b[0m                     \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_embedding_weight_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m                     \u001b[0muse_character_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_character_inputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m                     max_batch_size=self._max_batch_size)\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreuse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, options, weight_file, ids_placeholder, use_character_inputs, embedding_weight_file, max_batch_size)\u001b[0m\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariable_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bilm'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_build\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36m_build\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    281\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_char_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 283\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_word_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    284\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_lstms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36m_build_word_embeddings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    468\u001b[0m             self.embedding_weights = tf.get_variable(\n\u001b[1;32m    469\u001b[0m                 \u001b[0;34m\"embedding\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_tokens_vocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprojection_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 470\u001b[0;31m                 \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    471\u001b[0m             )\n\u001b[1;32m    472\u001b[0m             self.embedding = tf.nn.embedding_lookup(self.embedding_weights,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(name, shape, dtype, initializer, regularizer, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1295\u001b[0m       \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1296\u001b[0m       \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1297\u001b[0;31m       constraint=constraint)\n\u001b[0m\u001b[1;32m   1298\u001b[0m get_variable_or_local_docstring = (\n\u001b[1;32m   1299\u001b[0m     \"\"\"%s\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, var_store, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m   1091\u001b[0m           \u001b[0mpartitioner\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpartitioner\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1092\u001b[0m           \u001b[0muse_resource\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_resource\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1093\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m   1094\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1095\u001b[0m   def _get_partitioned_variable(self,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mget_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, custom_getter, constraint)\u001b[0m\n\u001b[1;32m    429\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m\"constraint\"\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mestimator_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcustom_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"constraint\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstraint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 431\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mcustom_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mcustom_getter_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    432\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m       return _true_getter(\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36mcustom_getter\u001b[0;34m(getter, name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    264\u001b[0m                 \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_weight_file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m             )\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mgetter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0membedding_weight_file\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_true_getter\u001b[0;34m(name, shape, dtype, initializer, regularizer, reuse, trainable, collections, caching_device, partitioner, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    406\u001b[0m           \u001b[0mtrainable\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    407\u001b[0m           \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 408\u001b[0;31m           use_resource=use_resource, constraint=constraint)\n\u001b[0m\u001b[1;32m    409\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    410\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcustom_getter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m_get_single_variable\u001b[0;34m(self, name, shape, dtype, initializer, regularizer, partition_info, reuse, trainable, collections, caching_device, validate_shape, use_resource, constraint)\u001b[0m\n\u001b[1;32m    798\u001b[0m         \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    799\u001b[0m         \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 800\u001b[0;31m         use_resource=use_resource)\n\u001b[0m\u001b[1;32m    801\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_store_eager_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    802\u001b[0m       \u001b[0;31m# In eager mode we do not want to keep default references to Variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mvariable\u001b[0;34m(initial_value, trainable, collections, validate_shape, caching_device, name, dtype, constraint, use_resource)\u001b[0m\n\u001b[1;32m   2155\u001b[0m                          \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2156\u001b[0m                          \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconstraint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2157\u001b[0;31m                          use_resource=use_resource)\n\u001b[0m\u001b[1;32m   2158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m   2145\u001b[0m              \u001b[0mconstraint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2146\u001b[0m              use_resource=None):\n\u001b[0;32m-> 2147\u001b[0;31m   \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mdefault_variable_creator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2148\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mgetter\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_variable_creator_stack\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2149\u001b[0m     \u001b[0mprevious_getter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_getter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgetter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_getter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36mdefault_variable_creator\u001b[0;34m(next_creator, **kwargs)\u001b[0m\n\u001b[1;32m   2128\u001b[0m         \u001b[0mcollections\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcollections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidate_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidate_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2129\u001b[0m         \u001b[0mcaching_device\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcaching_device\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2130\u001b[0;31m         constraint=constraint)\n\u001b[0m\u001b[1;32m   2131\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, variable_def, dtype, expected_shape, import_scope, constraint)\u001b[0m\n\u001b[1;32m    233\u001b[0m           \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m           \u001b[0mexpected_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexpected_shape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 235\u001b[0;31m           constraint=constraint)\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variables.py\u001b[0m in \u001b[0;36m_init_from_args\u001b[0;34m(self, initial_value, trainable, collections, validate_shape, caching_device, name, dtype, expected_shape, constraint)\u001b[0m\n\u001b[1;32m    335\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Initializer\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    336\u001b[0m               self._initial_value = ops.convert_to_tensor(\n\u001b[0;32m--> 337\u001b[0;31m                   initial_value(), name=\"initial_value\", dtype=dtype)\n\u001b[0m\u001b[1;32m    338\u001b[0m               shape = (self._initial_value.get_shape()\n\u001b[1;32m    339\u001b[0m                        if validate_shape else tensor_shape.unknown_shape())\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/variable_scope.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    782\u001b[0m           \u001b[0minitializer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    783\u001b[0m         init_val = lambda: initializer(  # pylint: disable=g-long-lambda\n\u001b[0;32m--> 784\u001b[0;31m             shape.as_list(), dtype=dtype, partition_info=partition_info)\n\u001b[0m\u001b[1;32m    785\u001b[0m         \u001b[0mvariable_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase_dtype\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/bilm_tf/bilm/model.py\u001b[0m in \u001b[0;36mret\u001b[0;34m(shape, **kwargs)\u001b[0m\n\u001b[1;32m    236\u001b[0m             raise ValueError(\n\u001b[1;32m    237\u001b[0m                 \"Invalid shape initializing {0}, got {1}, expected {2}\".format(\n\u001b[0;32m--> 238\u001b[0;31m                     varname_in_file, shape, weights.shape)\n\u001b[0m\u001b[1;32m    239\u001b[0m             )\n\u001b[1;32m    240\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Invalid shape initializing embedding, got [182714, 512], expected (182714, 300)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "\n",
    "sys.path.append(\"..\")\n",
    "    \n",
    "from bilm_tf.bilm import TokenBatcher, BidirectionalLanguageModel, \\\n",
    "weight_layers, dump_token_embeddings\n",
    "\n",
    "vocab_file = '/home/chenxin/ELMo/raw_5_vocab.txt'\n",
    "options_file = '/home/chenxin/ELMo/new_checkpoint/options.json'\n",
    "weight_file = '/home/chenxin/ELMo/new_checkpoint/weights.hdf5'\n",
    "token_embedding_file = '/home/chenxin/WordEmbedding/pre_trained_vectors/raw_300/vocab_embedding.hdf5'\n",
    "with open(options_file, 'r') as fin:\n",
    "    options = json.load(fin)\n",
    "\n",
    "## Now we can do inference.\n",
    "# Create a TokenBatcher to map text to token ids.\n",
    "batcher = TokenBatcher(vocab_file)\n",
    "\n",
    "# Input placeholders to the biLM.\n",
    "context_token_ids = tf.placeholder('int32', shape=(None, None))\n",
    "\n",
    "# Build the biLM graph.\n",
    "bilm = BidirectionalLanguageModel(\n",
    "    options_file,\n",
    "    weight_file,\n",
    "    use_character_inputs=False,\n",
    "    embedding_weight_file=token_embedding_file\n",
    ")\n",
    "\n",
    "# Get ops to compute the LM embeddings.\n",
    "context_embeddings_op = bilm(context_token_ids)\n",
    "config = tf.ConfigProto(allow_soft_placement=True)\n",
    "\n",
    "elmo_context_input = weight_layers('input', context_embeddings_op, l2_coef=0.0)\n",
    "with tf.variable_scope('', reuse=True):\n",
    "    # the reuse=True scope reuses weights from the context for the question\n",
    "    elmo_question_input = weight_layers(\n",
    "        'input', question_embeddings_op, l2_coef=0.0\n",
    "    )\n",
    "    \n",
    "elmo_context_output = weight_layers(\n",
    "    'output', context_embeddings_op, l2_coef=0.0\n",
    ")\n",
    "\n",
    "\n",
    "with tf.Session(config=config) as sess:\n",
    "    # It is necessary to initialize variables once before running inference.\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Create batches of data.\n",
    "    context_ids = batcher.batch_sentences(tokenized_context)\n",
    "\n",
    "    # Compute ELMo representations (here for the input only, for simplicity).\n",
    "    elmo_context_input_ = sess.run(\n",
    "        [elmo_context_input['weighted_op']],\n",
    "        feed_dict={context_token_ids: context_ids}\n",
    "    )\n",
    "\n",
    "print(elmo_context_input_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
